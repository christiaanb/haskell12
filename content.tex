\section{Introduction}

Simulation is often used to inspect various characteristics of a system.
In the \soos project\footnote{See \url{http://www.soos-project.eu/} and \cite{soos} for details.} we investigate service-oriented aspects of operating systems.
One part of the project requires to simulate the interactions between software components of an operating system (in the following: OS) and between
OS and application software.

% The key point is: it is not desirable to keep the whole, monolithic, OS kernel on each core of a multicore machine.
% Doing so has detrimental effects on the caches of the individual cores.\olcomment{find a better reference!}
% Moving to larger distributed systems, monolithic kernels hinder scalability as the entire internal state has to be synchronised amongst the copies.\jlcomment{Restate the paragraph after finding the reference!}
% \cbcomment{I don't think we should say more about S(o)OS or its ideas / concepts in the intro.}
% For example, we could dedicate some cores to an application and some cores to an OS.
% One possible approach to this is the partitioning of the address space. \olcomment{is this so?}
% In a simular manner, if we know that a core is occupied by a particular program, we can move the dynamic libraries, required for some other application to some other core's memory range.
% A further issue is the scalability of the parallel system.

%\olcomment{new: --- >}
Conventional operating systems often have multiple drawbacks due to their design.
The systems feature central synchronisation points for resource management and distribution; these hinder further scalability.
Current commonly available systems perform well on a handful of CPU cores, but they are not designed for thousands of cores!
(However, we want to mention the Barrelfish research project\nolinebreak[4]  \cite{Baumann:2009:MNO:1629575.1629579} featuring advanced ideas for manycore architectures.)
Heterogeneous (\eg GPU+CPU, Cell BE), non-cache coherent architectures (\eg Intel SCC) and network-on-a-chip approaches are quite poorly supported.
The emerging cloud computing era requires much greater scalability and security than we saw before; this requires new OS design directions.
%\olcomment{< --- new}

Overcoming the aforementioned limitations of contemporary operating system designs is the motivation for the \soos project \cite{soos}.
The project aims to research OS concepts and specific OS modules, which aid in scalability of the complete software stack (both OS and application) on future many-core systems.
One of the key concepts of \soos is that only those OS modules needed by an application thread, are actually loaded into the (local) memory of a CPU core on which the thread will run.
This execution environment thus differs from contemporary operating systems where every core runs a complete copy of the (monolithic) operating system.

While creating new operating system concepts and regarding their interaction with the programmability of large-scale systems, we found that existing simulation packages do not seem to have the right abstractions for fast design exploration \cite{cotson,omnet}.
The ability to simulate separate components of the OS and of the application was the main goal to develop the OS simulator \soosim \cite{Baaij:2012}.
The design decisions were:
\begin{itemize}
\item To simulate \emph{message passing} and \emph{shared memory} as the main forms of communication between software components.
\item Achieve \emph{synchronisation} between components through \emph{blocking} messaging.
\item To have global \emph{ticks}, designating an abstract notion of progress.
\item To observe the number of ticks a software component is active, waiting (for a response after invoking another component), or idle.
\end{itemize}
An idle component basically has an empty receive queue. 
% If a blocking message is received, the reviever should suspend its current work and process the message.\olcomment{is this true?}
A note on blocking messaging: when a blocking message is sent, the sender waits for an answer---regardless \emph{where} in the code of the sender the blocking \hs{invoke} method is called.
This means we required the notion of suspending a computation---it should be \emph{suspendable} at any moment of execution of the simulated component.
In the following we will show that using Haskell \cite{haskell-report} 
% \cbcomment{I don't think we need to cite the haskell report for the haskell symposium}
enabled us to implement this property in a nice way.
%The contributions of this paper include:
This paper features:
\begin{itemize}
\item A short overview of the \soosim simulator.
\item The description of the interface used to implement OS components in \soosim.
% \olcomment{Do \hs{createComponent} etc. really form a DSL? Say `interface'?}
% \cbcomment{I think DSL is wrong wording, interface is indeed better}
\item The description of \emph{an} eDSL to describe applications that will run on a \soosim \emph{simulated} system.
% \item \olcomment{in case of success: The way to model "suspending
%     computation" with heterogeneous data without using dynamic types.}
\item We reason on Haskell language features we found particularly
  useful in this project. \olcomment{+ reasoning WHY they are useful,
    + explanation what are they}
\end{itemize}
\soosim has been released on Hackage.\footnote{Issue \textsf{cabal install SoOSiM} to install \soosim. See also: \url{http://github.com/christiaanb/SoOSiM}.}
%% release is not a paper contribution, unfortunately!
\olcomment{what is our story? what do we want to tell?}  The goal of
this paper is to showcase how the sophisticated features from
programming language research repertoire were usable in a quite
applied and practical programming exercise.

The remaining part of this paper is organised as follows.
Section~\ref{sec:soosim-an-overview} describes the structure of the \soosim simulator from the user's perspective.
Section~\ref{sec:dsl} gives an implementation overview of the domain specific language, used to implement application running within a \soosim simulated environment.
Section~\ref{sec:impl-detail} discusses Haskell features from which our implementation especially benefited.
Section~\ref{sec:related-work} covers related work.
Section~\ref{sec:concl-future-work} concludes and gives an outlook for the further research.

\section{\soosim: An Overview}
\label{sec:soosim-an-overview}

\paragraph{Basic structure.}
The purpose of \soosim is to provide a platform that allows a developer to observe the interactions between OS modules and application threads.
For this reason the simulated hardware is highly abstract.

In \soosim, the hardware platform is described as a set of nodes.
Each \emph{node} represents a physical computing object: such as a core, a complete CPU, a memory controller, etc.
Every node has a local memory of potentially infinite size.
The layout and connectivity properties of the nodes are not part of the system description.
% If such a level of detail is required, it would have to be modelled explicitly by the user.
% \jlcomment{How?}
% \olcomment{Is it possible to model these within \soosim, using various DSL implementations? If yes, say "In our design, embedded DSLs facilitate such definitions. See Section ... for details."}
% \cbcomment{By creating a component, which would reside on each node, that handles the messaging.
% Meaning that if you want to communicate with a component, you would invoke the messenger component, instead of the component itself.
% The messenger module would then keep an internal state, representing the connectivity of all the nodes, and forward the message accordingly.}
%% nice, but not now. maybe useful for an extended version

Each node hosts a set of components.
A \emph{component} represents an executable object: such as a thread, application, OS module, etc.
Components communicate with each other either using direct messaging, or through the local memory of a node.
Having both explicit messaging and shared memory, 
% \olcomment{memory or memories?}
\soosim supports the two most well known methods of communication.
Components also have a message queue, because:
\begin{itemize}
  \item Multiple components can potentially send messages to the same component concurrently.
  \item A component can receive messages while it is actually waiting for a response from another component.
\end{itemize}
All components in a simulated system, even those hosted within the same node, are executed concurrently, from the component's point of view.
The simulator poses neither restrictions as to which components can communicate with each other nor to which node's local memory they can read from and write to.\olcomment{why?}
A schematic overview of an example system can be seen in Figure~\ref{fig:system}.

%\def\svgwidth{\columnwidth}
\begin{figure}
\centering
%\includesvg{system}
\includegraphics[width=0.7\linewidth]{system}
\caption{System, abstracted.}
%\vspace{-1em}
\label{fig:system}
\end{figure}

\paragraph{Agility.}
Basic requirements that we would have towards any simulator include the facilities to straightforwardly simulate instantiating of application threads and OS modules.
Aside from the fact that the \soos-envisioned system will be dynamic as a result of loading OS modules on-the-fly, large-scale systems also tend to be dynamic in the sense that computing nodes can disappear (failure), or
appear (hot-swap).
Hence, we also require that our simulator facilitates the straightforward creation and destruction of computing elements, represented by \emph{nodes} in \soosim.
Our current need for a simulator rests mostly in formalising the \soos concept, and examining the interaction between our envisioned OS modules and the application threads.
As such, being able to extract highly accurate performance figures from a simulated system is not a key requirement.
We do, however, wish to be able to observe all interactions among application threads and OS modules.
Additionally, we wish to be able to \emph{zoom in} on particular aspects of the behaviour of an application: such as memory access, messaging, etc.
We detail on this in Section~\ref{sec:dsl}.

The simulator \emph{attempts} to progress all components concurrently in one discrete step called a \emph{tick}.
If a component is waiting for a response from an invoked component, but none is available, a component will remain in a waiting state.
During a tick, the simulator progresses a component by one of the following means:
\begin{itemize}
  \item If a component is blocked on an invocation of another component, and a response is available in the message queue, the component will be executed with this response message.
  \item If a component is not blocked, and the message queue is not empty, the component will be executed by passing it the message that is at the head of the message queue.
  \item If a component is not blocked, and the message queue is empty, the component will be executed by passing a \emph{null} message it.
\end{itemize}
Null messages facilitate component execution:
when desired, a component can inform the simulator that it does not want to receive these null messages.
In this case the component will not be executed by the simulator during a \emph{tick}.

The simulator keeps several statistics with regards to the components, including how many ticks a component was either: active, waiting, or idle.
When a component makes progress during a tick, it is considered \emph{active}.
When a component does not make progress because it is blocked on an invocation, and no response was available, the component is counted as \emph{waiting}.
A component not making progress because it has an empty message queue, and having indicated that it does not want to receive \emph{null} messages, is designated as \emph{idle}.

\subsection{OS Component Descriptions}
Like the simulator itself, the OS components are also specified in Haskell. Every component is modelled as a function.
In case of \soosim, such a function is executed within the context of
the simulator, this means: in a monad.

%\olcomment{is this a good place for a defintion?}  
A \emph{monad} can be defined with a Kleisli triple: combining a type constructor \hs{m}, a unit function \hs{return  ::  a -> m a}, and a bind function \hs{>>=  ::  m a -> (a -> m b) -> m b}, we obtain a monad.
Monads capture the way of combining particular operations; the witty description is `programmable semicolon'.

Because the function is executed within the monad, it can have \emph{side-effects} such as sending messages to other components, or reading the local memory of a node.
In addition, the function can be temporarily suspended at (almost) any point in the code.
\soosim needs to be able to suspend the execution of a function so that we are able to emulate synchronous message passing between components, which we will return to later.

We describe a component as a function that receives a user-defined internal state as its first argument and a value of type \hs{SimEvent} as its second argument.
The result of this function is the internal state.
A value of type \hs{SimEvent} is either a message from another component or a null message:
\begin{code}
data SimEvent
  =  Message ComponentId Dynamic
  |  Null
\end{code}
We thus have the following \emph{general} type signature for a component:
\begin{code}
component :: State -> SimEvent -> SimM State
\end{code}

The simulator monad \hs{SimM} is described in detail in
Figure~\ref{fig:code-simm} in
Section~\ref{sec:impl-monads}.\olcomment{forward reference, BAD}
The user-defined internal \hs{state} can be used to store any information that needs to perpetuate across simulator ticks.
The type \hs{State} should be read as a placeholder for any (user-defined) data type; there is no actual predefined \hs{State} data type.

To include a component description in the simulator, the developer will have to create an instance of the \hs{ComponentIface} \emph{type class}.
% A \emph{type class} in Haskell can be compared to an interface
% definition as those known in object-oriented languages.  An
% \emph{instance} of a \emph{type class} is a concrete instantiation
% of such an interface.
%% haskell people know that!
The \hs{ComponentIface} requires the instantiation of the following values to completely define a component:

\begin{itemize}
  \item The initial internal state of the component.
  \item The unique name of the component.
  \item The monadic function describing the behaviour of the component.
\end{itemize}

We stress again that we are aiming at a high level of abstraction for the behavioural descriptions of our OS modules, where the focus is mainly on the interaction with other OS modules and application threads.


\subsection{Interaction with the Simulator}

Components have several functions at their disposal to interact with the simulator and consequently interact with other components.
The available functions are:
\begin{itemize}
\item {\hs{createComponent}}
instantiates a new component on a specified node.
\item {\hs{invoke}}
sends a message to another component and waits for the answer.
Whenever a component uses this function it will be temporarily suspended by the simulator.
Several simulator ticks might pass before the response.
Once the response is available the simulator resumes the execution of the calling component.
\item {\hs{invokeAsync}}
sends a message to another component and registers a handler with the simulator to process the response.
In a contrast to \hs{invoke}, using this function will \emph{not} suspend the execution of the component.
\item {\hs{respond}}
sends a message to another component as a response to an invocation.
\item {\hs{yield}}
informs the simulator that the component does not want to receive null messages.
\item {\hs{readMem}}
performs a read at a specified address of a node's local memory.
\item {\hs{writeMem}}
writes a value at a specified address of a node's local memory.
\item The function {\hs{componentLookup}}
performs a look-up of the unique identifier of a component on a specified node.
\end{itemize}
Components have two unique identifiers, their global \emph{name} (as specified in the \hs{ComponentIface} instance), and a \hs{ComponentId} that is a unique number corresponding to a specific instance of a component.
When a programmer wants to \emph{invoke} a component, she needs to know the unique \mbox{\hs{ComponentId}} of the specific instance.
To give a concrete example, using the system from Figure~\ref{fig:system} as our context: \emph{Thread (\#6)} wants to invoke the instance of the \emph{Memory Manager} that is running on the same Node (\#2).
As \emph{Thread (\#6)} was not involved with the instantiation of that OS module, it has no idea what the specific \hs{ComponentId} of the memory manager on Node \#2 is.
% Was: It knows the unique global name of all memory managers, so it can use the \hs{componentLookup} function to find the \hs{Memory Manager} with ID \#5 that is running on Node \#2.
It knows the unique global name of the memory manager in question, so it can use the \hs{componentLookup} function to find the \hs{Memory Manager} with ID \#5 that is running on Node \#2.
%\olcomment{is this correct?}\cbcomment{yes, it is}

\subsection{Synchronisation between Modules}
A \emph{naive} approach to model synchronisation between OS modules is to implement an OS module as a finite state machine (FSM).
In state `X' OS module `Alice' could send a message to `Bob' and enter state `Y'.
Only when receiving a response from `Bob', would `Alice' progress from state `Y' to state `Z'.
If we wanted, we could even have implemented OS modules as \emph{pure} functions, which, aside from returning an updated state, would also return a simulator event.
One such event would then be sending a message to another component.

When sending a message the FSM would enter a new state for that specific invocation of another component.
As a result, designers of OS module descriptions would have to extend the dictionary of states for \emph{every} invocation their module makes.
As one can imagine, the explicit FSM approach would quickly result in overly verbose and unstructured OS module descriptions.

As a solution we essentially mechanised the creation of the FSM using the notion of \emph{suspendable} computations.
When a developer uses the \hs{invoke} function, two things actually happen:
\begin{itemize}
  \item A message is send to the invoked component (callee).
  \item The remaining computation of the caller is suspended and stored by the simulator, ready to be scheduled once a response to the invocation arrives.
\end{itemize}
Every call to \hs{invoke} hence corresponds to an explicit state transition, but without the burden of explicit state-keeping.
We achieved suspendability using coroutines~\cite{coroutines}, more specifically, a coroutine monad transformer~\cite{cmt}.
We return to the subject of the coroutine monad transformer in Section~\ref{sec:impl-detail}.

\section{A Domain-Specific Language for \soosim}
\label{sec:dsl}
One of the purposes of \soosim is to observe the interaction between application and OS.
We however do not want to \emph{pollute} our application descriptions with calls to the simulator functions.
Nor do we want to be forced to describe our application in a monadic style.
Additionally, we want to be in control which interactions between OS and application are exactly simulated, without having to change either the descriptions of the OS modules or the application when we want to observe different aspects.

For the above reasons we have decided to use embedded languages for the definitions of our applications.
The idea is to specify an application once in a self-created embedded language and to define different interpretations for the embedded language constructs as the observations of the interaction between application and OS\nolinebreak[3]  \cite{Hofer:2008:PED:1449913.1449935}.

Following the \emph{final tagless} \cite{final_tagless_embedding} encoding of embedded languages in Haskell, we use a type class to define the language constructs.
As an example we demonstrate a mini functional language with mutable references in Figures~\ref{fig:edsl-mutref} and~\ref{fig:edsl-inst}.
%\olcomment{was: "running example".}
% A partial specification of the \hs{Symantics} (a pun on \emph{syntax} and \emph{semantics} \cite{final_tagless_embedding}) type class, defining an \emph{embedded language}, is shown in Figure~\ref{fig:embedded_language_interface}.
The arrow \hs{:->} is the function arrow in the embedded language, \eg \hs{repr (a :-> b)}, in contrast to the function arrow \hs{->} of the host language.

% \begin{figure}
% \olcomment{old}
% \begin{code}
% class Symantics repr where
%   lam   :: (repr a -> repr b) -> repr (a :-> b)
%   app   :: repr (a :-> b) -> repr a -> repr b

%   drf   :: repr (Ref a) -> repr a
%   (=:)  :: repr (Ref a) -> repr a -> repr Void
% \end{code}
% %\vspace{-0.5em}
% \caption{An embedded language, a fragment. Viz.~\cite{final_tagless_embedding}}
% %\vspace{-1em}
% \label{fig:embedded_language_interface}
% \end{figure}


\begin{figure}
%\olcomment{new DSL}
\begin{code}
class EDSL exp where
     lam :: (exp a -> exp b) -> exp (a :-> b)
     lam = lamS

     lamS :: (exp a -> exp b) -> exp (a :-> b)
     app :: exp (a :-> b) -> exp a -> exp b

     lt :: exp IntT -> exp IntT -> exp BoolT
     if_ :: exp BoolT -> exp a -> exp a -> exp a

     ref :: exp a -> exp (Ref a)
     deref :: exp (Ref a) -> exp a
     update :: exp (Ref a) -> exp a -> exp UnitT
-- ...
\end{code}
\caption{An eDSL with mutable references}
\label{fig:edsl-mutref}
\end{figure}

% \begin{figure}
% \olcomment{new example: fib with mutable vars!}
% \begin{code}
% infixl 2 $$ -- similar to host's ($)

% let_ x y = (lam y)  `app` x
% letS x y = (lamS y) `app` x

% newvar x y = letS (ref x) y
% x >: y     = letS x (\_ -> y)

% infixr 5 >:

% fix' = lam $ \f ->
%          newvar (lam $ \r -> undefined) $ \r ->
%            update r (lam $ \x -> f $$ (deref r) $$ x) >:
%            deref r

% fix f = app fix' (lam f)

% fibbo = fix $ \fib ->
%   lam $ \n ->
%     newvar 0 $ \n1 ->
%     newvar 0 $ \n2 ->
%     newvar 0 $ \n3 ->
%       update n1 n >:
%       if_ (lt (deref n1) 2)
%         1
%         ( update n2 (app fib ((deref n1) - 1)) >:
%           update n3 (app fib ((deref n1) - 2)) >:
%           (deref n2) + (deref n3)
%         )
% \end{code}%$
% \caption{Computing Fibonacci numbers in a langauge with mutable
%   references. The langauge itself is defined in Figure~\ref{fig:edsl-mutref}}
% \label{fig:edsl-fib}
% \end{figure}

\begin{figure}
\olcomment{WTF is \hs{Sem} and \hs{Sim}? Explain!}
\begin{code}
newtype S m a = S { unS :: m (Sem m a) }

type SState = StateT Int Sim

share :: SState a -> SState (SState a)
share = ... -- implementation omitted

instance EDSL (S SState) where -- partially shown
  lamS f       = S . return $ (\x -> x >>= unS . f . S . return)
  lam f        = S . return $ (\x -> share x >>= unS . f . S)
  app x y      = S $ unS x >>= ($ (unS y))

-- ref and deref implementation massage the state
\end{code}
\caption{One of the instances of the EDSL type class}
\label{fig:edsl-inst}
\end{figure}
% One interaction we might now want to observe is the application's use of mutable references and the subsequent communication with the memory manager OS module.
% For this purpose we create a \hs{newtype RefMemAcc} wrapping the simulator monad, and define an instance of the \hs{Symantics} type-class for it.

We can define most language constructs to behave like their Haskell counterpart, e.g., the \hs{app} construct instance is defined using monadic bind (\hs{>>=}\xspace).
However, for the definition of the mutable reference aspect of our
embedded language, we implement these constructs as % invocations of
                                % the memory manager (see
                                % Figure~\ref{lst_observing_memory_access}),
state manipulations, 
instead of using a Haskell-based implementation like \textsf{Data.IORef}.

% \begin{figure}
% \olcomment{depricated?}
% \begin{code}
% newtype RefMemAcc =  RMA SimM

% instance Symantics RefMemAcc where

%   drf x = RMA $ do
%     i     <-- foo x
%     mmId  <-- componentLookup "MemoryManager"
%     fmap unmarshal $ invoke mmId (marshal (Read i))

%   x =: y = RMA $ do
%     i     <-- foo x
%     v     <-- bar y
%     mmId  <-- componentLookup "MemoryManager"
%     fmap unmarshal $ invoke mmId (marshal (Write i v))
% \end{code}
% %\vspace{-0.5em}
% \caption{Observing memory access (partial definition).}
% %\vspace{-1em}
% \label{lst_observing_memory_access}
% \end{figure}


\paragraph{Final tagless embedding.}
\citeauthor{final_tagless_embedding} have demonstrated how a typed interpreter of a typed language can be tagless \cite{final_tagless_embedding}.
This approach does not use GADTs or dependent types.
The target language types are represented as host language types, object terms are encoded as calls to host language functions. With type classes \citeauthor{final_tagless_embedding} make it possible to express, \eg an interpreter and a compiler of the same language in a flexible manner.
\citeauthor{Hofer:2008:PED:1449913.1449935} do a similar work \cite{Hofer:2008:PED:1449913.1449935}, but focus on different interpretations.
This matches our intentions.
We use our above example of a small language with mutable references.
If we want to observe memory interactions in more detail, we embrace an additional implementation of the language, where the functions for the memory interaction are exactly studied, leaving out of consideration other parts of the detailed language implementation.
In a similar manner we can introduce a further implementation of the same language with different properties.

\section{Features We Used}
\label{sec:impl-detail}
Now we discuss language features that contributed significantly to the
\soosim implementation. We use existential types, type classes,
monads, coroutines, and dynamic types. We decided against using
parameterised monads and a further usage of existential types, as
detailed in Section~\ref{sec:param-monads-disc}.
\subsection{Existential Types}
%\olcomment{definition?}
We employ existential types to store the differently typed states of
all the components running in our simulated environment; consult
\citeauthor{CambridgeJournals:1350264}
\cite{CambridgeJournals:1350264} for details.
Components are subsequently implemented as type class instances, giving us a straightforward way to access the existentially quantified values in our collection of states.

\subsection{Type Classes}
Type classes \cite{Hall:1996:TCH:227699.227700} are naturally used for
the eDSL encoding \`a la \citeauthor{final_tagless_embedding} from the
previous section \cite{final_tagless_embedding}.

Beyond this, we use type classes to express many other useful abstractions.
For instance, an \emph{interface} for the component of the OS is a type class.
Thus, each interacting `building block' in the simulated software is an instance of that particular type class.
Although initially used solely for the purpose of dealing with existentially quantified values, this \hs{ComponentIface} type class enables great flexibility, including an option to extend the kinds of simulated objects by third party.

\subsection{Monads}
\label{sec:impl-monads}
\begin{figure}[tb]
%\centering
%format ^^ = "\ \ "
\begin{code}
type SimMonad  =  StateT SimState IO
data SimState  = ...

newtype SimM a
  = SimM { runSimM ::
      ^^ ^^ Coroutine
      ^^ ^^ ^^ ^^ (RequestOrYield ComponentID Dynamic)
      ^^ ^^ ^^ ^^ SimMonad
      ^^ ^^ ^^ ^^ a 
    } deriving (Functor, Monad)

data RequestOrYield request response x
  =  Request request (response -> x)
  |  Yield   x

instance Functor (RequestOrYield x f) where
  fmap f (Request x g)  = Request x (f . g)
  fmap f (Yield y)      = Yield (f y)
\end{code}
%\vspace{-0.5em}
\caption{Implementing \hs{SimM}.}
%\vspace{-1.5em}
\label{fig:code-simm}
\end{figure}


We use a monad called \hs{SimM} to capture the \emph{state} of the simulator.
The implementation of \hs{SimM}\hspace{-1pt}, sketched in Figure~\ref{fig:code-simm}, enables us to reach the main implementation goal: the ultimate suspension and resumption of the components upon message passing.
%% negative hspace to move the coma!
To suspend a computation in the current component we can now merely write \hs{request componentId}, where the \hs{componentId} is the unique ID of some other component.
The suspending of a component is nothing else than sending an outgoing message in the name of the component and blocking the sender until a response.
To continue the execution of a resumable computation we issue \hs{resume} \mbox{\hs{computation}.} %% hackery to get spaces right


\subsection{Coroutines}
\label{sec:impl-coroutines}
As we mentioned above and as one could infer from Figure~\ref{fig:code-simm}, the implementation uses coroutines~\cite{coroutines}.

Coroutines are fragments of a program, much like subroutines. 
However, a coroutine can be suspended at a particular point and entered again from this point. 
Basically, a thread yielding to scheduler implements a similar behaviour.

With this concept we naturally capture the notion of multiple suspending and resuming computations.
The need in such a behaviour arises naturally with the blocking message passing between components.
We utilise \textsf{Control.Monad.Coroutine} \cite{cmt} to express coroutines.

With coroutines we can express blocking in a natural and distributed manner.
No other components need to be notified, the suspending function is called after sending a synchronous message, the resume function is called after receiving an answer.

\subsection{Dynamic Types}
The dynamic types \cite{Abadi:1991:DTS:103135.103138} allow to combine
a heterogeneously typed values under a single `umbrella' type, and
subsequently to restore initial types. Such a behaviour disrupts to
the certain degree the static type system of Haskell.

We use dynamic types % as `the other side'\olcomment{a reviewer does
                     % not like this!} of the type classes:
because 
we need to communicate heterogeneous types over a typed channel, and there is no option to know all the types-to-transmit at the moment of the channel definition.

The \emph{origin} for the need of dynamic types lies in the suspension functor of our coroutines, more specifically, in the \hs{response} field, that designates the type of the response returned after an invocation (see Figure~\ref{fig:code-simm}).
This field would be different for every component we want to invoke, as components should be able to communicate data of any type. %% kind is the type of types, was not meant here as such, I assume
We chose to encode this aspect in the suspension functor using dynamic types.

% \cbcomment{The use-case for existentials+type-classes is to get an OO-like way of programming; please correct me if I'm wrong.
% Just take a look at: http://www.haskell.org/haskellwiki/Existential_type
% This approach is perfect if you know upfront what kind of operations you want to perform on the data-types.
% The wiki demonstrates this with an example of a type-class that determines the perimeter and area of different datatypes representing geometric shapes.
% And indeed, you can put different shapes in a homogeneous collection, and calculate the area and perimeter for the different types of shapes in this collection.

% The use case in SoOSiM is however far different: we do NOT know, upfront, how the data-types will be used.
% The information stored in these values can and should be be manipulated in any kind of way.
% Trying to define a type-class 'Foo' would either be too large, or too restrictive.

% So I really think that 'Dynamic' is the 'answer' to our problem, although it is an ugly one.}

If we knew upfront what kind of operations the users might want to perform on the data types, we could have used existential types and type classes, \viz \cite{Laufer:1994:PTI:186025.186031,Kiselyov:2004:STH:1017472.1017488}.
However, this is not our case.
% \olcomment{I connect the dynamic types to communication, as we need the coroutines for blocking!}
We need to synchronously transmit values of heterogeneous types.
% and the information on how these values should be used is stored in the values themselves!
% This information should be be manipulated in any kind of way.
The types in the coroutine monad transformer are too restrictive.
Trying to define a type class or a sum type for this setting would yield either a too large, or a too restrictive definition; trying to care for every possible usage we would either overdo it, or limit  future applications.

The `list' of the types we can send via a single channel should be extendable for the user, hence we cannot use a sum type.
Using a type class relieves this problem, but forces constraints on the behaviour of its instances.
If we would try to keep this type class as generic, as possible, we would obtain a reimplementation of \textsf{Data.Dynamic}.
If we make the type class more special, is might be of a benefit for particular kind of simulated tasks, but would endanger the generality of the implementation.
An attempt to sort the dynamic types out with the parameterised monad is detailed in the next section.

%We are not aware of any other way to implement this behaviour.
%


Hence, we chose the most straightforward approach; we utilised \textsf{Data.Dynamic} to convert transmitted values of a dynamic type into values of concrete monomorphic types.
With it, we can transmit heterogeneously typed values over a monomorphicly typed channel.
The types are restored after the transition with the facilities of \textsf{Data.Dynamic}.
A similar problem was faced by \citeauthor{grace-tfp} when building a graph-based tool for parallel message-passing programs, they used \hs{unsafeCoerce} \cite{grace-tfp}.

\subsection{Parameterised Monads --- Discussion}
\label{sec:param-monads-disc}
Parameterised monads\footnote{See also \url{http://www.haskell.org/pipermail/haskell-prime/2006-February/000498.html}.}  allow more generic type signatures for monad operations, esp.\ for \hs{>>=}\xspace\nolinebreak[4] \cite{CambridgeJournals:8240527,Ghani:2005:MAG:1090189.1086403,monad-parametrisable}:
% Atkey:2009:PNC:1561017.1561022,
\begin{code}
class PMonad m where
  return :: a -> m s s a
  (>>=)  :: m s1 s2 a -> (a -> m s2 s3 b) -> m s1 s3 b
\end{code}
If we could switch from one  intermediate type to another in the
coroutine, we would need no dynamic types to implement \soosim.
This approach works fine with the state monad
\begin{code}
newtype PState s1 s2 a
  = PState { runPState :: s1 -> (a, s2) }

instance PMonad PState where
  return a  = PState $ \s -> (a, s)
  m >>= k   = PState $ \s ->
                ^^ ^^ let (a, s') = runPState m s
                ^^ ^^ in  runPState (k a) s'
\end{code}
but it essentially fails for the coroutines.
The pivotal issue is: if we introduce different types for the intermediate states of the coroutine, we end up adding more and more types --- the coroutine definition is recursive.
In this (non proper Haskell) code
\begin{code}
newtype Coroutine m o i r
  =  Coroutine { resume ::
       ^^ ^^  m (Either
       ^^ ^^ ^^ ^^ ^^ ^^ (o, i -> Coroutine m i2 o r)
       ^^ ^^ ^^ ^^ ^^ ^^ r ) }
\end{code}
we make an attempt to reach our goal.\olcomment{what about existential
types here?}
Let us abbreviate left and right hand side of the above equation with LHS and RHS correspondingly.
However, to make this code valid Haskell, we need to reference \hs{i2} on the LHS: \hs{Coroutine m i i2 o r}.
In the type of the \hs{resume} function on the RHS we need to reference \hs{Coroutine} correctly, hence we write \hs{Coroutine m i2 i3 o r} there.
We need to add \hs{i3} on the LHS, and so on.
% Using some more advanced tricks, like a type class for all the chain \hs{i}, \hs{i2}, \hs{i3}, \dots\ does not really help: all the types on the RHS still need to be listed on the LHS, thus exploding the definition.
% \olcomment{check this again!}
\olcomment{new --->}
We can begin fixing this, using existential types: for a suitable type class
\hs{Intermediate} we can write
%format <.> = "\cdot"
\begin{code}
data Coroutine m o i i2 r
        =  forall i3 <.> (Intermediate i3) 
           =>  Coroutine (resume :: m (Either
                                      (o, i -> Coroutine m i2 i3 o r)
                                      r ))
\end{code}
However, an instance of \hs{PMonad} is not very
straightforward. We loose the ability to use \hs{resume} as a
function. Further drawbacks are:
\begin{itemize}
\item We need to use a non-standard monad definition, the \hs{PMonad}.
\item We are require a re-implementation of coroutines; we cannot use
  the standard implementation from \textsf{Control.Monad.Coroutine}.
\item The readability of the source code suffers.
\end{itemize}
Hence, we decided to stick to the initial approach with \textsf{Data.Dynamic}.

\section{Related Work}
\label{sec:related-work}

\paragraph{Operating systems and simulators.}
COTSon \cite{cotson} is a full system simulator that allows a developer to execute normal x86 code in a simulated environment.
COTSon is far too detailed for our needs, and does not facilitate the easy exploration of a complete operating system.

OMNeT++ \cite{omnet} is a C++-based discrete event simulator with focus on parallel systems. OMNeT++ is too static for our purposes, it disallows dynamic modules.

\citeauthor{house} describe a basic operating system implementation in Haskell called House \cite{house}.
This work is in a sense dual to ours: we simulate an OS.
\citeauthor{house} modified GHC run-time system to allow code execution on bare metal.
In House, OS modules are executed within the \hs{Hardware} monad, allowing direct interaction with real hardware.
This approach is comparable to our \hs{SimM} monad.
However, as we are more concerned with interaction of OS modules, House is not suitable for our purposes:  OS modules in House must be implemented in full detail.

Barrelfish \cite{Baumann:2009:MNO:1629575.1629579} is an OS in which domain-specific languages are used, amongst other purposes, to define driver interfaces \cite{barrelfish}.
These embedded languages are also implemented in Haskell.
The approach used in Barrelfish is however to create parsers for their languages, in a contrast to our embedding approach.


\paragraph{Embedded domain-specific languages.}
Embedding of programming languages was proposed by \citeauthor{Landin:1966:NPL:365230.365257} \cite{Landin:1966:NPL:365230.365257}.
For the concept of an eDSL see the papers by \citeauthor{hudak1} \cite{hudak1,hudak2}, the 1998 paper describes pure embedding.
The literature on this topic is quite broad, an overview is presented, \eg by \citeauthor{dsl-survey} \cite{dsl-survey}.

The final tagless embedding of DSLs originates from the seminal paper by \citeauthor{final_tagless_embedding} \cite{final_tagless_embedding}.
\citeauthor{Hofer:2008:PED:1449913.1449935} \cite{Hofer:2008:PED:1449913.1449935} presented a paper that compared to the work by \citeauthor{final_tagless_embedding} \cite{final_tagless_embedding} is more composable, allows subtyping, and is done in Scala \cite{odersky2008programming}.
These two papers have different goals: \citeauthor{final_tagless_embedding} implement DSLs for higher-order languages, while \citeauthor{Hofer:2008:PED:1449913.1449935} focus on the possibility to plug in multiple interpretations of a pure embedded DSL.
The latter corresponds more with our goals: we want to control  what we can observe in the particular instance.

\paragraph{Suspendable computation.} We used coroutine monad transformer \cite{cmt} to express a suspendable computation.
Other approaches to this include the suspension monad implementations of iteratees/enumeratees. This approach was introduced in \cite{oleg-iteratee}, see, \eg an overview by \citeauthor{lato2010iteratee} \cite{lato2010iteratee}.
See also Oleg Kiselyov's new paper \cite{springerlink:10.1007/978-3-642-29822-6_15} for the current description of iteratee-based IO in Haskell.

% \paragraph{Haskell experience reports.}
% Haskell has been successfully used to implement a range of applications far from typical programming language research.
% This is not surprising for a general purpose language, but nevertheless we would like to list most interesting applications.
% We already mentioned House \cite{house} in the previous section.

% Haskell has been used to facilitate a formal verification of a
% microkernel \cite{Klein:2009:ERS:1631687.1596566}, to implement
% commercial mission-critical applications
% \cite{Sampson:2009:ERH:1631687.1596578}, to manage a Linux
% distribution \cite{Beshers:2007:ERU:1291220.1291184}, to configure
% realtime OS components \cite{Jones:2008:ERP:1411203.1411219}, etc.


\section{Conclusions and Future Work}
\label{sec:concl-future-work}

\paragraph{Conclusions.}
We have presented the efforts required to implement \soosim, an OS simulator in Haskell.
We note that the techniques more commonly used in the programming language research were highly applicable for our purposes.

We have demonstrated how the utilisation of final tagless eDSL construction \cite{final_tagless_embedding,Hofer:2008:PED:1449913.1449935}, type classes \cite{Hall:1996:TCH:227699.227700}, monads \cite{Wadler:1990:CM:91556.91592}, \hs{Dynamic} types \cite{Abadi:1991:DTS:103135.103138}, and coroutines \cite{coroutines,cmt} facilitated an abstract and concise implementation of an operating system simulator.
It is uncommon to use all these features for such a task in practise --- a fact that emphasises the novelty of our work.

\paragraph{Real life simulations.}
The major goal of the project is to simulate the behaviour of a real life application and to draw conclusions therefrom.
We are currently actively researching these issues.

\paragraph{Concurrency.}
One aspect of the future work is the concurrent \soosim.
One option is to use Concurrent Haskell \cite{ConcHs}, it provides (concurrent) green threads.
However, we would also need to use software transactional memory
\cite{springerlink:10.1007/s004460050028} because of multiple writes
to the same data. (Some other options, like locks are also viable, but
less well supported in Haskell.)
Examples of fruitful combinations of this concept with other ones include papers by \citeauthor{Harris:2008:CMT:1378704.1378725} \cite{Harris:2008:CMT:1378704.1378725} and \citeauthor{Bieniusa:2010:BAA:1835698.1835714} \cite{Bieniusa:2010:BAA:1835698.1835714,springerlink:10.1007/978-3-642-25959-3_2}.
Another option would be to use a \emph{parallel} Haskell like Multicore Haskell \cite{marlow:rsm}, \hs{Par} monad \cite{par-monad} or Eden \cite{eden}.
The `multiple writes' issue, however, needs some further handling in this case.
Two further options are either to make the coroutine execution concurrent using actors~\cite{Hewitt:1973:UMA:1624775.1624804,sulzmann2008actors} or to switch to iteratees for expressing blocking communication and to utilise transformers from usual to parallel composable iteratees\footnote{See \url{http://projects.haskell.org/pipermail/iteratee/2011-July/000083.html} for details.}.

% \appendix
% \section{Appendix Title}
%
% This is the text of the appendix, if you need one.

%\acks
\paragraph{Acknowledgements.}
%Acknowledgments, if needed.
This work was supported by the \soos project, sponsored by the European Commission under FP7-ICT-2009.8.1, Grant Agreement No.~248465.
We thank Chung-chieh Shan and Oleg Kiselyov for valuable remarks on
\hs{Dynamic} types and the parallel iteratees correspondingly.
We also thank Tommaso Cucinotta for a fruitful discussion.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "soosim"
%%% End:
